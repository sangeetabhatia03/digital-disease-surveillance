---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
title: Cleaning up the feed from HealthMap/ProMED
author:
- name: Sangeeta Bhatia
  affiliation: Imperial College London
abstract: 
keywords: 
date: "`r Sys.Date()`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
bibliography: 
biblio-style: apsr
endnote: no
params:
  datasource: HealthMap
  infile: data/raw/HealthMap_Ebola_GNE_WHO.csv
  ofinterest: data/processed/all_african_centroids.csv
---


```{r data-cleanup-1, include=FALSE}
opts_chunk$set(
  dev = "pdf",                      
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.path = "figures/"
)
```


```{r setup}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(glue)
library(janitor)
library(lubridate)
library(purrr)
library(readr)
library(scales)
library(tidyr)
```


```{r data-cleanup-2 }
species <- "Humans"
disease <- "Ebola"
wafrica <- c("Sierra Leone", "Liberia", "Guinea")
datasource <- "promed"
outfile <- glue(
  "{Sys.Date()}_{datasource}_loglinear"
)
```

Visualising the raw data.



```{r data-cleanup-3 }

feed <- read.csv(
  "data/raw/promed_2014-2016-renamed.csv",
  stringsAsFactors = FALSE
) %>%
  filter(Species == species &
         Disease == disease &
         Country %in% wafrica)

feed <- clean_names(feed)
## Janitor renames healthmap to health_map. Fix.
feed <- rename(feed,
  healthmap_alert_id = health_map_alert_id
)
```

Strip the date of the time stamp

```{r data-cleanup-4 }
feed <- separate(feed,
  issue_date,
  sep = " ",
  into = c("issue_date", "time"),
  remove = TRUE
)
```

Split the data by country.

```{r data-cleanup-5 }
by_location <- split(feed, feed$country)
```

### Raw data by case category

```{r data-cleanup-6 }
## Deaths are not used anywhere. No use plotting them
x <- select(feed, issue_date, sc, cc, country) %>%
  gather(
    case_type, count, -c(issue_date, country)
  )

x$issue_date <- as.Date(x$issue_date, format = "%m/%d/%y")
p <- ggplot(x, aes(issue_date, count, col = case_type)) +
    geom_point(size = 1.1) +
    facet_wrap(~country, scales = "free_y", ncol = 1) +
    scale_x_date(date_labels = "%d-%m-%Y") +
    scale_color_manual(
      values = c(sc = "#00BFC4", cc = "#F8766D"),
      breaks = c("sc", "cc"),
      labels = c("Suspected Cases", "Confirmed Cases")
    ) + ylab("Cases") 
  p <- p + theme_minimal() +
    theme(
      panel.border = element_blank(),
      axis.title.x = element_blank(),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      legend.title = element_blank(),
      legend.position = "top"
    )
  
p

```


# Data clean-up

Extract the total case count as a sum of suspected and
confirmed cases.


```{r data-cleanup-8 }
sum_only_na_stays_na <- function(x){
  if (all(is.na(x))) out <- NA
  else out <- sum(x, na.rm = TRUE)
  out
}

cum_cases <- map_dfr(
  by_location, function(case_count) {
  case_count$date <- mdy(case_count$issue_date)
  case_count$cases <- apply(
    case_count[, c("sc", "cc")], 1, sum_only_na_stays_na
  )
  case_count
})

```

## Merge duplicate alerts

```{r data-cleanup-9 }
cols_to_keep <- c(
  "location", "country", "disease", "species",
  "healthmap_alert_id", "headline", "url",
  "alert_tag", "feed_name", "lon", "lat"
)
## These are columns we generate ourselves later on
cols_to_keep <- c(cols_to_keep, "date", "cases")

```

```{r data-cleanup-10 }
no_dups <- merge_duplicates(cum_cases, cols_to_keep)
```
## Compare before and after removal of duplicates

```{r}
x <- bind_rows(
  list("Without duplicates" = no_dups[, c("date","country", "cases")],
       "With duplicates" = cum_cases[, c("date","country", "cases")]),
  .id = "var"
)
p <- ggplot(x, aes(date, cases, col = var)) +
    geom_point() +
    facet_wrap(~country, scales = "free_y", ncol = 1) +
    scale_x_date(date_labels = "%d-%m-%Y") +
  ylab("Cases")

p <- p + theme_minimal() +
  theme(
    panel.border = element_blank(),
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.title = element_blank(),
    legend.position = "top"
  )
p
```

## Remove outliers. 

```{r data-cleanup-11 }
use_last <- 20
p.within.k <- 0.50
k_sd <- mRIIDS:::interval_width_for_p(
  use_last,
  1 - p.within.k
) %>%
  sqrt() %>%
  `[`(2)

no_outliers <- map(no_dups, function(df) {
  df <- arrange(df, date)
  df <- filter(df, !is.na(cases)) %>% select(date, cases)
  if (nrow(df) == 0) {
    return(NULL)
  } else {
    df <- mRIIDS:::remove_last_outliers(df,
      use_last = use_last,
      k_sd = k_sd
    )
  }
  message(df$country[1])
  df
})
```

## Make monotonically increasing

```{r data-cleanup-12 }

mono_inc <- split(no_dups, no_dups$country) %>%
  map(make_monotonically_increasing)

```

## Interpolate missing data
Finally, interpolate missing data.

```{r data-cleanup-13 }
interpolated <- map(
  mono_inc,
  ~ interpolate_missing_data(., method = "loglinear")
)
```


Check that each dataframe within interpolated corresponds to daily 
series.

```{r data-cleanup-14}
map(interpolated, function(x) diff(x$date))
```

## Derive incidence time series from interpolated cumulative cases

Stan is not going to like real numbers for a Poisson process.
So first we cast the cumulative case counts as integers and then 
take the difference.


```{r data-cleanup-15 }

incid_tall <- map_dfr(interpolated, function(x) {
  x <- select(x,
    date = interpolated_date,
    cases = interpolated_cases
  )
  x$cases <- as.integer(x$cases)
  x$incid <- c(0, diff(x$cases))
  x <- select(x, date, incid)
  x
}, .id = "country")

```

## Plot


### Total cases as sum of sc and cc.

Common plotting logic. x must have a date and a cases column.

```{r data-cleanup-17 }

cases_ts <- function(x) {
  p <- ggplot(x, aes(date, cases)) +
    geom_point(size = 0.5)
  p <- p + theme_minimal()
  p <- p + xlab("") + ylab("Cases")
  p <- p + scale_x_date(date_labels = "%d-%b-%Y")
  p
}

```

```{r data-cleanup-18 }
total_p <- cases_ts(cum_cases) +
  facet_wrap(~country, scales = "free_y", ncol = 1) + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
    
```

### Remove duplicate alerts

```{r data-cleanup-19 }
nodups_p <- cases_ts(no_dups) +
  facet_wrap(~country, scales = "free_y", ncol = 1) + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

```
### Make increasing

```{r data-cleanup-21 }
monoinc_p <- map(mono_inc, cases_ts)
monoinc_p <- map(
  monoinc_p, function(p) {
    p <- p + theme(axis.text.x = element_text(hjust = 0.5))
    p
})     

```

### Interpolated data

This needs a slightly different viz.

```{r data-cleanup-22, eval = TRUE}

interp_p <- map(interpolated, function(x) {
  x$src <- glue("{datasource}.FALSE")
  idx <- which(is.na(x$cases))
  x$src[idx] <- glue("{datasource}.TRUE")
  x <- select(x,
    date = interpolated_date,
    cases = interpolated_cases,
    src
  )
  p <- ggplot(x, aes(date, cases, col = src)) + geom_point(size = 0.5)
  p <- p + xlab("") + ylab("Cases") 
  p <- p + theme_minimal()
  p <- p + theme(
    axis.text.x = element_text(hjust = 0.5),
    panel.border = element_blank(),
    axis.line = element_line(),
    legend.position = "top"
  )
  p <- p + xlab("") + theme(legend.title = element_blank())
  p <- p + scale_x_date(date_labels = "%d-%b-%Y")
  p
})

```
## Daily Incidence Series

```{r data-cleanup-23}
daily_bycountry <- split(incid_tall, incid_tall$country)

names(daily_bycountry) <- countrycode::countrycode(
  names(daily_bycountry), "iso3c", "country.name"
)

daily_p <- map(
    daily_bycountry,
    function(df) {
      p <- ggplot(df, aes(date, incid)) +
        geom_point(size = 0.5)
      p <- p + xlab("") + ylab("Daily incidence")
      p <- p + theme_minimal()
      p <- p +
        theme(
          axis.text.x = element_text(hjust = 0.5),
          panel.border = element_blank(),
          axis.line = element_line()
        )
      p <- p + scale_x_date(date_labels = "%d-%b-%Y")
      p

    }    
)

```
Putting them together.

```{r data-cleanup-24, eval = TRUE}

countries <- names(by_location)
graphs <- list(
  raw_p,
  total_p,
  nodups_p,
  nooutss_p,
  monoinc_p,
  interp_p,
  daily_p
)

walk(
    countries,
    function(c) {
        cspecific <- map(graphs, ~ .x[[c]])
        names(cspecific) <- 1:length(cspecific)
        iwalk(
            cspecific,
            function(p, idx) {
                filename <- here::here(
                   all_files[[datasource]]$outdir, 
                   glue("{outfile}_{c}_{idx}.pdf")
                   )
                ggplot2::ggsave(
                    plot = p,
                    filename = filename,
                    width = mriids_plot_theme$single_col_width,
                    height = mriids_plot_theme$single_col_height,
                    units = mriids_plot_theme$units
                )                
                
            }    
        )
    }
  )



```


Also, get rid of the first row of all 0s to get Stan started.

```{r data-cleanup-25 }

incid_tall <- filter(
  incid_tall,
  date != "2014-03-21"
)
```
And reshape.

```{r data-cleanup-26 }
incid_wide <- tidyr::spread(incid_tall, country, incid, fill = 0)
```


## Add 0 incidence for other countries

Finally, we add 0 incidence for all countries other than ones for
which we already have data. 

```{r data-cleanup-27 }
ofinterest <- readr::read_csv(here::here(params$ofinterest)) %>%
  filter(include == "TRUE")

nodata <- setdiff(
  ofinterest$ISO3,
  countrycode::countrycode(
    wafrica,
    "country.name",
    "iso3c"
  )
)
df <- matrix(0,
  nrow = nrow(incid_wide),
  ncol = length(nodata)
)
colnames(df) <- nodata
df <- data.frame(df)
incid_wide_all <- cbind(incid_wide, df)
```

Finally ensure that the order of countries in centroids and incid 
files is the same.

```{r data-cleanup-28, eval = TRUE}
inorder <- ofinterest$ISO3
## Some of the countries of interest may not be present in
## incid_wide_all
inorder <- intersect(inorder, colnames(incid_wide_all))
incid_wide_all <- select(
  incid_wide_all,
  date,
  inorder
)
```


## Write the output

```{r data-cleanup-29 }



  readr::write_csv(
    x = incid_wide_all,
    path = here::here(
        all_files[[datasource]]$outdir,
        glue("{outfile}_wide.csv")
    )
  )
```
 
## Also make note of what points were interpolated.

```{r data-cleanup-30 }
incid_extracols <- map_dfr(
  interpolated, function(x) {
  idx <- which(is.na(x$cases))
  x$interpolated <- "FALSE"
  x$interpolated[idx] <- "TRUE"

  x <- select(
    x,
    date,
    interpolated
  )
},
.id = "country"
)

incid_extracols$country <- countrycode::countrycode(
  incid_extracols$country,
  "country.name",
  "iso3c"
)
```

Reshaping from tall to wide and back to tall allows us to get the same
dates for all countries. We do this for both data frames and then 
join the tall forms of both.

```{r data-cleanup-31}
incid_tall <- tidyr::gather(
    data = incid_wide,
    key = country,
    value = incid,
    -date
)
## Check
## group_by(incid_tall, country) %>% summarise(n = n())
# A tibble: 6 x 2
##   country     n
##   <chr>   <int>
## 1 GIN       656
## 2 LBR       656
## 3 MLI       656
## 4 NGA       656
## 5 SEN       656
## 6 SLE       656

## When reshaping to wide, we get NAs for dates for which
## we don't have data for a given country. This we had set to 0 in
## reshaping incid; So we set interpolated to FALSE for these dates.
out <- tidyr::spread(
    incid_extracols,
    country,
    interpolated,
    fill = FALSE
    )
## Reshaping back to tall
incid_extracols <- tidyr::gather(
    data = out,
    key = country,
    value = interpolated,
    -date
  )    
## Check
## group_by(incid_extracols, country) %>% summarise(n = n())
# A tibble: 6 x 2
##   country     n
##   <chr>   <int>
## 1 GIN       657
## 2 LBR       657
## 3 MLI       657
## 4 NGA       657
## 5 SEN       657
## 6 SLE       657

```
Now we can join the two.

```{r data-cleanup-32 }
## dim(incid_tall) 2310    3
## dim(incid_extracols) 2311    3
## The 1 row (Ghana) that we deleted from incid_tall
incid_tall <- left_join(
    incid_tall,
    incid_extracols
    )
## group_by(incid_tall, country) %>% summarise(n = n())
# A tibble: 6 x 2
##   country     n
##   <chr>   <int>
## 1 GIN       656
## 2 LBR       656
## 3 MLI       656
## 4 NGA       656
## 5 SEN       656
## 6 SLE       656
## This should not have introduced NAs. 
## incid_tall[!complete.cases(incid_tall), ]
## [1] country      date         incid        interpolated
## <0 rows> (or 0-length row.names)

```


```{r data-cleanup-33 }
## This is wrong; see notes
## incid_extracols <- left_join(
##   incid_tall,
##   incid_extracols,
##   by = c(
##     "date",
##     "country"
##   )
##)

df <- cbind(date = incid_wide_all$date, df)
df_tall <- tidyr::gather(
  df,
  country,
  incid,
  -date
)
## group_by(df_tall, country) %>% summarise(n = n()) %>% filter(n != 656)
## A tibble: 0 x 2
## … with 2 variables: country <chr>, n <int>

## By the same logic as before, we set the flag to FALSE for these
## countries.

df_tall$interpolated <- FALSE

incid_tall <- rbind(
  incid_tall,
  df_tall
)

## group_by(incid_tall, country) %>% summarise(n = n()) %>% filter(n != 656)
## # A tibble: 0 x 2
## # … with 2 variables: country <chr>, n <int>

## This should not have introduced NAs. 
## incid_tall[!complete.cases(incid_tall), ]
## [1] country      date         incid        interpolated
## <0 rows> (or 0-length row.names)




readr::write_csv(
    x = incid_tall,
    path = here::here(
        all_files[[datasource]]$outdir,
        glue("{outfile}_tall.csv")
    ) 
  )
```

## Weekly series

Add a flag to indicate if all points in the week were reconstructed.

```{r data-cleanup-34 }

## No NAs at this point
## incid_extracols[is.na(incid_extracols$incid), ]

## Not all countries have the same number of rows.
## group_by(incid_extracols, country) %>% summarise(n()) %>% filter(`n()` != 656)
# A tibble: 6 x 2
##   country `n()`
##   <chr>   <int>
## 1 GIN       584
## 2 LBR       566
## 3 MLI        70
## 4 NGA       531
## 5 SEN        69
## 6 SLE       490
## Essentailly we had the following number of 0 incidence rows missing from our
## analysis for sensitivity and specificity.
##   country      newcols
##   <chr>          <dbl>
## 1 Guinea            71
## 2 Liberia           90
## 3 Mali             586
## 4 Nigeria          125
## 5 Senegal          587
## 6 Sierra Leone     166

## Why work on incid_extracols? This will lead to missing all those
## other countries we added.
extra <- as.numeric(max(incid_tall$date) -
  min(incid_tall$date)) %% 7


dates <- unique(sort(incid_tall$date))
  

remove <- tail(dates, extra)

daily <- filter(
  incid_tall,
  !(date %in% remove)
)
daily$interpolated <- as.logical(daily$interpolated)
## Check for NAs
## any(is.na(daily$interpolated))
weeks <- cut(daily$date, breaks = "7 days")
weekly <- split(daily, weeks) %>%
  map_dfr(~ group_by(
    .x,
    country
  ) %>%
    summarise(
      incid = sum(incid),
      interpolated = all(interpolated)
    ),
  .id = "date"
  )
## weekly[!complete.cases(weekly), ]
## group_by(weekly, country) %>% summarise(n()) %>% filter(`n()` != 94)
## A tibble: 0 x 2
## … with 2 variables: country <chr>, `n()` <int>
## Check that the numbers look ok.



readr::write_csv(
  x = weekly,
  path = here::here(
    all_files[[datasource]]$outdir,
    glue("{outfile}_weekly.csv")
  )
)

outfile <- paste0(outfile, "_no_dups.csv")

bind_rows(no_dups) %>%
  select(country, date, cases) %>%
  readr::write_csv(
  path = here::here(
    all_files[[datasource]]$outdir,
    glue("{outfile}_no_dups.csv")
  )
 )

```
